resources:
  - apiVersion: slurm.nebius.ai/v1alpha1
    kind: ActiveCheck
    metadata:
      name: all-reduce-perf-nccl
      namespace: ${slurm_cluster_namespace}
    spec:
      name: all-reduce-perf-nccl
      checkType: slurmJob
      suspend: false
      reactions:
        drainSlurmNode: true
        setCondition: true
      runAfterCreation: true
      schedule: 0 4,12,20 * * * # 3 times a day at 4:00, 12:00, and 20:00 UTC
      slurmClusterRefName: ${slurm_cluster_name}
      slurmJobSpec:
        eachWorkerJobArray: true
        jobContainer:
          image: cr.eu-north1.nebius.cloud/soperator/slurm_check_job:1.21.10-jammy-slurm24.11.5
          env:
            - name: K8S_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: K8S_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - mountPath: /mnt/jail
              name: jail
          volumes:
            - name: jail
              persistentVolumeClaim:
                claimName: jail-pvc
        mungeContainer:
          image: cr.eu-north1.nebius.cloud/soperator/munge:1.21.5-jammy-slurm24.11.5
        sbatchScript: |
          #!/bin/bash

          #SBATCH --deadline="now+6hours"
          #SBATCH --time=10:00
          #SBATCH --mem=32G
          #SBATCH --gpus-per-node=8
          #SBATCH --cpus-per-task=16

          echo "Checking for running GPU processes..."

          if [[ -n "$(nvidia-smi --query-compute-apps=pid --format=csv,noheader | grep -v '^ *$')" ]]; then
            echo "Another GPU process is currently running. Exiting."
            exit 0
          fi

          platform=""
          gpus_on_node=$(nvidia-smi --query-gpu=name --format=csv,noheader | sort | uniq -c)

          if [[ "$${gpus_on_node}" == *"8 NVIDIA H100"* ]]; then
            platform="8xH100"
          elif [[ "$${gpus_on_node}" == *"8 NVIDIA H200"* ]]; then
            platform="8xH200"
          elif [[ "$${gpus_on_node}" == *"8 NVIDIA B200"* ]]; then
            platform="8xB200"
          else
            echo "Unsupported platform"
            exit 0
          fi

          echo "Platform found: $platform"
          echo "Running all_reduce_without_ib check on $(hostname)..."

          HC_OUTPUT=$(srun --cpu-bind=verbose,cores bash -c "health-checker run -e soperator -p $platform -n all_reduce_without_ib --json-log")
          HC_EXIT_CODE=$?

          echo "Health checker output: $HC_OUTPUT"
          echo "Health checker job step exit code: $HC_EXIT_CODE"

          HC_STATUS=$(echo "$HC_OUTPUT" | awk '/^\s*{/,/^\s*}/' | jq -r '.status')

          echo "Health checker status: $HC_STATUS"

          if [[ "$HC_STATUS" == "ERROR" && $HC_EXIT_CODE -eq 1 ]]; then
            echo "Health-checker reported status=ERROR and exited with non-zero status."
            exit 1
          else
            echo "Health-checker passed or returned non-ERROR status."
            exit 0
          fi
